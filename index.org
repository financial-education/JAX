#+title: JAX
#+author: Matt Brigida, Ph.D.
#+email: matthew.brigida@sunypoly.edu
#+PROPERTY: header-args :eval no
#+SETUPFILE: https://fniessen.github.io/org-html-themes/org/theme-readtheorg.setup
#+HTML_HEAD: <style>pre.src{background:#343131;color:white;} </style>

JAX is a young ML ecosystem that provides simple tools than can be composed to solve complex problems.  It provides Numpy-like functionality on the GPU with **grad** and vectorization methods.  Below we'll use JAX to estimate a market model.

#+begin_comment
## try linear reg from here:
## https://coax.readthedocs.io/en/latest/examples/linear_regression/jax.html
#+end_comment

Load libraries:

#+BEGIN_SRC python :exports both :session *jax* :results none
import pandas as pd
import yfinance as yf
import jax
import jax.numpy as jnp
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
#+end_src

Get data from Yahoo Finance, convert them into returns, and split into training and test data sets.

#+BEGIN_SRC python :exports both :session *jax* :results none
market = yf.Ticker("SPY")
market_data = market.history(period="1y")

target = yf.Ticker("GME")
target_data = target.history(period="1y")

market_returns = market_data["Close"].pct_change()[1:]
target_returns = target_data["Close"].pct_change()[1:]

X, X_test, y, y_test = train_test_split(market_returns.values, target_returns.values)
#+end_src

Model

#+BEGIN_SRC python :exports both :session *jax* :results none
# model weights
params = {
    'w': 1., #jnp.zeros(X.shape[1:]),
    'b': 0.
}


def forward(params, X):
    return jnp.dot(X, params['w']) + params['b']  # calculates y_hat


def loss_fn(params, X, y):
    err = forward(params, X) - y  # y_hat - y
    return jnp.mean(jnp.square(err))  # mse


grad_fn = jax.grad(loss_fn)


def update(params, grads):
    return jax.tree_multimap(lambda p, g: p - 0.9 * g, params, grads)


# the main training loop
for _ in range(50000):
    loss = loss_fn(params, X_test, y_test)
#    print(loss)

    grads = grad_fn(params, X, y)
    params = update(params, grads)
#    print(params)
#+end_src

#+RESULTS:



#+BEGIN_SRC python :exports both :session *jax* :results output
print(params)
#+end_src

#+RESULTS:
: {'b': DeviceArray(0.00072088, dtype=float32, weak_type=True), 'w': DeviceArray(2.2803657, dtype=float32, weak_type=True)}

This has a beta of src_python[:exports results :session *jax* :results output]{print(round(params['w'], 2))} {{{results(=2.28=)}}}, which is reasonable for GME.


Check results

#+BEGIN_SRC python :exports both :session *jax* :results output
from sklearn.linear_model import LinearRegression
reg = LinearRegression().fit(X.reshape(-1,1), y)
print(reg.coef_)
#+end_src

#+RESULTS:
: [2.28085816]

So a beta of src_python[:exports results :session *jax* :results output]{print(round(reg.coef_[0], 2))} {{{results(=2.28=)}}}, looks good.


